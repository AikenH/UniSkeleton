val_n: 1
apex: false
cuda_devices: "0"

resume: false
save_pth: save_model
ckpt_pth: save_model/pre_train/mlp_cifar_rs18/68_1024_simclr.pt
metrics_type: !!python/tuple ["acc1", "acc5"]

logger:
  logname: train_cifar100_resnet50
  console_level: 2
  prefix: null # set null will get the time as prefix
  log_dir: ./log # this default setting we usually donot change it
  subfix: B_

# !! the num of class should be control by the dataset part
networks:
  {
    feat_model: cifar_rs50,
    cls_model: mlp,
    num_cls: 80,
    hidden_layers: null,
    pretrain: false,
    in_dim: 2048,
    feature_vis: true,
    activation: null,
    dropout: 0.5,
  }

# istrain:0 train | 1 test  | else val
data:
  # now we support full(new+imb), imb, new  3 way to define the data or null
  preprocess: both
  transformer: cifar100
  dataset:
    {
      datatype: cifar100,
      save_path: ../DownloadData,
      istrain: 0,
      needMap: false,
      num_new: 20,
      imb_factor: 0.1,
      strategy: step,
      # 0.1->10 0.02->50, 0.01->100
      # imb_factor: 0.1, 
      # strategy: exp,
      decay: 0.5,
      sort: false,
      num_cls: 100,
      random_seed: 888,
    }

pretrain:
  ispretrain: false
  using_projector: true
  diff_lr: false
  pretrain_opt:
    { type: load, pth: save_model/pre_train/mlp_cifar_rs50_cifar100/1024_81.pt}
  epoch: 500
  # the num_cls is the projector's out dim
  network:
    {
      feat_model: cifar_rs50,
      cls_model: mlp,
      num_cls: 128,
      hidden_layers: !!python/tuple [512, 512],
      pretrain: false,
      in_dim: 2048,
      feature_vis: false,
      activation: null,
    }

  dataset:
    {
      datatype: cifar100,
      save_path: ../DownloadData,
      istrain: 0,
      num_cls: 100,
      random_seed: 888,
    }

  dataload_opt:
    {
      batch_size: 1024,
      shuffle: true,
      num_workers: 28,
      pin_memory: false,
      drop_last: true,
      prefetch_factor: 5,
    }

  loss_opt: { LOSSTYPE: ntxent, memory_size: 0, temperature: 0.07 }
  optim: SGD
  optim_opt: { lr: 0.01, momentum: 0.9, weight_decay: 0.0005 }
  # sche_opt: { milestones: !!python/tuple [100, 160, 260], gamma: 0.2 }
  sche_opt: {mode: max, factor: 0.2, patience: 10, min_lr: !!float 1e-8, verbose: true}


training_opt:
  loss: CE
  notzeroloss: 0.12
  optimizer: SGD
  optim_opt: { lr: 0.1, momentum: 0.9, weight_decay: 0.005}
  # optim_opt: {lr: 0.1, betas: !!python/tuple [0.9,0.999], eps: !!float 1e-8, weight_decay: 0.1}
  schedule: auto
  # sche_opt: { milestones: !!python/tuple [60, 110, 180, 240, 300], gamma: 0.5}
  sche_opt: {mode: max, factor: 0.5, patience: 10, min_lr: !!float 1e-8, verbose: true}
  freq_out: 20
  train: { num_epochs: 350, drop_rate: 0.5, patience: 40, iterations: 100, verbose: true}
  dataopt:
    {
      batch_size: 512,
      shuffle: true,
      sampler: null,
      num_workers: 28,
      collate_fn: null,
      pin_memory: false,
      drop_last: true,
      prefetch_factor: 5,
    }
  criterion: { alpha: 0, beta: 0 }

online:
  enable: false
  resume: false
  ckpt: save_model/distill/mlp_cifar_rs18_cifar100/2021-12-14_09-24-44_PM.pt
  cluster_t: kmeans
  cluster_dict: { n_clusters: 20, init: k-means++, random_state: 0 }
  downsample_t: pca
  downsample_dict: { n_components: 30, random_state: 0}
  projecter: false
